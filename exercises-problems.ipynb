{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Connecting to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an iPython notebook.  You can execute a cell by clicking on it and pressing shift-enter.\n",
    "\n",
    "We can execute spark commands in here directly and get immediate results.\n",
    "\n",
    "When you open up a notebook with `pys`, you automatically have a variable, `sc`, available.  This is a Spark Context.  It's our starting point for all Spark operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x104239ad0>\n"
     ]
    }
   ],
   "source": [
    "print sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be using Python with DataFrames, which is only available in Spark 1.3 or later.  We're going to be using a recent version of open source spark.  To use it, you'll have to import the `SQLContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.SQLContext object at 0x1054c0090>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sql = SQLContext(sc)\n",
    "print sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Cassandra Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: string, address: string, age: int, city: string, gender: string, name: string, occupation: string, zip: string]\n"
     ]
    }
   ],
   "source": [
    "users = sql.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=\"movielens_small\", table=\"users\")\n",
    "print users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying results\n",
    "\n",
    "If we never perform an operation, our dataframe is never read in.  We can force our dataframe into memory and see it by calling `collect()` or `show()` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'bd8dab96-c2f2-4200-830d-4a44abf376ec', address=u'458 Britton Mews', age=19, city=u'New Katina', gender=u'M', name=u'Chanel Lesch', occupation=u'student', zip=u'02146')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.limit(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+---------------+------+--------------------+----------+-----+\n",
      "|                  id|             address|age|           city|gender|                name|occupation|  zip|\n",
      "+--------------------+--------------------+---+---------------+------+--------------------+----------+-----+\n",
      "|bd8dab96-c2f2-420...|    458 Britton Mews| 19|     New Katina|     M|        Chanel Lesch|   student|02146|\n",
      "|7ccfd064-d902-402...|03155 Bartoletti ...| 46|    Kesslerberg|     M|     Alyvia Walsh MD| librarian|77008|\n",
      "|9ce825e1-e803-4ee...|    8515 Taryn Inlet| 47|     Jaslynport|     M|       Caesar Ledner|technician|Y1A6B|\n",
      "|32d6ba75-335d-42d...|69008 Cruickshank...| 30|      Lake Mona|     M|Miss Tristan Schi...|technician|29379|\n",
      "|3f43f023-37bc-4b4...|7391 Bartoletti M...| 28|    Gerholdland|     M|           Elna Hahn|  engineer|20770|\n",
      "|4d653d07-1190-4e0...|  59325 Murazik Rest| 43|     Tobinville|     M|Miss Shanika Schn...| librarian|02324|\n",
      "|35f64a87-8640-43e...|57568 Hessel Bran...| 24|       Huelfort|     M|         Allyn Jones|   student|55454|\n",
      "|05bada47-e349-407...|367 Boyle Row Sui...| 18|    Crooksville|     M|Ms. Paulette Kerluke|   student|97520|\n",
      "|9758d1f3-864c-4a6...|  600 Bernhard Route| 50|     South Joey|     F|        Meagan Hoppe|     other|10016|\n",
      "|3f439ead-24fa-4c3...|     253 Kiara Track| 16|North Wayneberg|     F|     Brooklyn Jacobs|   student|49705|\n",
      "+--------------------+--------------------+---+---------------+------+--------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Filtering\n",
    "\n",
    "If we're going to do anything with our data, we need to be able to do a simple task: Filtering.\n",
    "\n",
    "Here's the syntax for filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+-----------+------+---------------+----------+-----+\n",
      "|                  id|             address|age|       city|gender|           name|occupation|  zip|\n",
      "+--------------------+--------------------+---+-----------+------+---------------+----------+-----+\n",
      "|7ccfd064-d902-402...|03155 Bartoletti ...| 46|Kesslerberg|     M|Alyvia Walsh MD| librarian|77008|\n",
      "+--------------------+--------------------+---+-----------+------+---------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.filter(users.age > 20).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an alternative syntax for filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, address: string, age: int, city: string, gender: string, name: string, occupation: string, zip: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user[user.age > 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, a third syntax for filters that have a degree of complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+--------------+------+--------------+----------+-----+\n",
      "|                  id|             address|age|          city|gender|          name|occupation|  zip|\n",
      "+--------------------+--------------------+---+--------------+------+--------------+----------+-----+\n",
      "|c6d0504b-cae5-41e...|43500 Metro Villa...| 53|  South Harden|     F|Dani Traphagen|     other|94043|\n",
      "|c62f9926-2be5-47b...|561 Maegan Garden...| 52|     Eldontown|     M| Daniel Glover|  educator|93109|\n",
      "|836c22ea-7187-4b7...|97642 Luigi Row S...| 60|West Dilanfort|     M| Dani Schiller|   retired|95076|\n",
      "+--------------------+--------------------+---+--------------+------+--------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user.filter(\"name LIKE 'Dani%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try filtering for users named \"Jon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting specific columns\n",
    "\n",
    "When you only want to see specific fields in a DataFrame, you will use the `select()` method.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.select(users.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll want to use a different name for a field than is in the original DataFrame.  For that, you'll want to know about `.alias()`.  For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, years: int]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.select(users.name, users.age.alias(\"years\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have a pipeline of DataFrame queries, and need to do a filter, you'll need to either temporarily assign the intermediate DataFrames to a variable or you'll need to use the SQL syntax.  For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-883227d27a90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"years\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"years > 10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'user' is not defined"
     ]
    }
   ],
   "source": [
    "users.select(user.name, users.age.alias(\"years\")).filter(\"years > 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                name|years|\n",
      "+--------------------+-----+\n",
      "|        Chanel Lesch|   19|\n",
      "|     Alyvia Walsh MD|   46|\n",
      "|       Caesar Ledner|   47|\n",
      "|Miss Tristan Schi...|   30|\n",
      "|           Elna Hahn|   28|\n",
      "|Miss Shanika Schn...|   43|\n",
      "|         Allyn Jones|   24|\n",
      "|Ms. Paulette Kerluke|   18|\n",
      "|        Meagan Hoppe|   50|\n",
      "|     Brooklyn Jacobs|   16|\n",
      "|      Missie Yost MD|   18|\n",
      "|      Elbridge Swift|   23|\n",
      "|     Hettie Wiza DDS|   27|\n",
      "|      Amil Wisozk MD|   48|\n",
      "|  Penni O'Conner PhD|   26|\n",
      "|       Basil Labadie|   25|\n",
      "|      Deliah McGlynn|   28|\n",
      "|  Mrs. Shay Anderson|   27|\n",
      "|Miss Chanel Pfeff...|   20|\n",
      "|       Seneca Schoen|   68|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = users.select(users.name, users.age.alias(\"years\"))\n",
    "tmp[tmp.years > 10].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Select Expressions\n",
    "\n",
    "Select expressions allow you to perform various SQL-like operations on your data, still in the JVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|old_age|\n",
      "+-------+\n",
      "|    190|\n",
      "|    460|\n",
      "|    470|\n",
      "|    300|\n",
      "|    280|\n",
      "|    430|\n",
      "|    240|\n",
      "|    180|\n",
      "|    500|\n",
      "|    160|\n",
      "|    180|\n",
      "|    230|\n",
      "|    270|\n",
      "|    480|\n",
      "|    260|\n",
      "|    250|\n",
      "|    280|\n",
      "|    270|\n",
      "|    200|\n",
      "|    680|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.selectExpr(\"age * 10 as old_age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convenience functions\n",
    "When working with DataFrames you'll frequently need access to some convenience functions.  For instance, `explode()` is use when you're working with sets and lists.  It creates 1 row per item in the set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies = sql.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=\"movielens_small\", table=\"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|        col|                name|\n",
      "+-----------+--------------------+\n",
      "|Documentary|Fire on the Mount...|\n",
      "|     Action|Rumble in the Bro...|\n",
      "|  Adventure|Rumble in the Bro...|\n",
      "|      Crime|Rumble in the Bro...|\n",
      "|    Romance|         Diva (1981)|\n",
      "|    Mystery|         Diva (1981)|\n",
      "|   Thriller|         Diva (1981)|\n",
      "|      Drama|         Diva (1981)|\n",
      "|     Action|         Diva (1981)|\n",
      "|      Drama|Magic Hour, The (...|\n",
      "|     Comedy|      Kingpin (1996)|\n",
      "|     Comedy|    Sleepover (1995)|\n",
      "|      Drama|    Sleepover (1995)|\n",
      "|      Crime|L.A. Confidential...|\n",
      "|  Film-Noir|L.A. Confidential...|\n",
      "|    Mystery|L.A. Confidential...|\n",
      "|   Thriller|L.A. Confidential...|\n",
      "|     Comedy|Raising Arizona (...|\n",
      "|      Drama|     Infinity (1996)|\n",
      "|     Comedy|Kicked in the Hea...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "movies.select(explode(movies.genres), movies.name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For queries like the above, it's useful to use our alias command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|       food|                name|\n",
      "+-----------+--------------------+\n",
      "|Documentary|Fire on the Mount...|\n",
      "|     Action|Rumble in the Bro...|\n",
      "|  Adventure|Rumble in the Bro...|\n",
      "|      Crime|Rumble in the Bro...|\n",
      "|    Romance|         Diva (1981)|\n",
      "|    Mystery|         Diva (1981)|\n",
      "|   Thriller|         Diva (1981)|\n",
      "|      Drama|         Diva (1981)|\n",
      "|     Action|         Diva (1981)|\n",
      "|      Drama|Magic Hour, The (...|\n",
      "|     Comedy|      Kingpin (1996)|\n",
      "|     Comedy|    Sleepover (1995)|\n",
      "|      Drama|    Sleepover (1995)|\n",
      "|      Crime|L.A. Confidential...|\n",
      "|  Film-Noir|L.A. Confidential...|\n",
      "|    Mystery|L.A. Confidential...|\n",
      "|   Thriller|L.A. Confidential...|\n",
      "|     Comedy|Raising Arizona (...|\n",
      "|      Drama|     Infinity (1996)|\n",
      "|     Comedy|Kicked in the Hea...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.select(explode(movies.genres).alias(\"food\"), movies.name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: When you refer to `movies.genres`, you're looking at a `Column`.  The api for `Column is here: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column\n",
    "\n",
    "**Advanced Query:** Try selecting the movies who have the genre \"Drama\".  You'll need to use `explode()`, `alias()` and a filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A nicer reader\n",
    "\n",
    "Personally I find needing to code `org.apache.spark.sql.cassandra` everywhere a little annoying.  Here's a couple convenience functions that returns a function (slightly tricky) that can be used to reference tables in a keyspace.  Execute the below block.  You can then refer to tables like such:\n",
    "\n",
    "`user = reader(\"user\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_reader(sql):\n",
    "    def reader(table):\n",
    "        df = sql.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=\"movielens_small\", table=table)\n",
    "        return df\n",
    "    return reader\n",
    "\n",
    "def create_writer(sql, mode=\"append\"):\n",
    "    def writer(df, table):\n",
    "        df.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "                 options(table=table, keyspace=\"movielens_small\").save(mode=\"append\")\n",
    "    return writer\n",
    "\n",
    "writer = create_writer(sql)\n",
    "reader = create_reader(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Migrations\n",
    "\n",
    "One thing Spark is useful for is performing data migrations.  We frequently need to take a table and write out a new structure.  Here's an example where we take the movie table and construct a new table that maps genres to movies.  The `writer()` function takes a dataframe and a table. \n",
    "\n",
    "Create this table in CQLSH:\n",
    "\n",
    "```\n",
    "CREATE TABLE movies_by_genre (\n",
    "  genre text,\n",
    "  id uuid,\n",
    "  name text,\n",
    "  avg_rating float,\n",
    "  primary key(genre, id)\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies_by_genre = movies.select(\"id\", \"name\", \"avg_rating\", explode(movies.genres).alias(\"genre\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer(movies_by_genre, \"movies_by_genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn.  This migration may be a little tricky.  What we want is a leaderboard where we can quickly view the top movies in a given genre.  \n",
    "\n",
    "```\n",
    "CREATE TABLE movie_leaderboard (\n",
    "  genre text,\n",
    "  avg_rating float,\n",
    "  id uuid,\n",
    "  name text,\n",
    "  primary key (genre, avg_rating, id)\n",
    ") with clustering order by (avg_rating desc);\n",
    "```\n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL\n",
    "\n",
    "The programatic interface above is pretty convenient, and in my opinion, fun.  There's another interface that's very convenient if you come from a SQL background: SparkSQL.  SparkSQL supports quite a bit of Hive's SQL dialect.\n",
    "\n",
    "You can register a table to query with SQL like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users.registerTempTable(\"users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try registering your movies DataFrame as `movies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How's your SQL?  You can execute queries against the temp tables you've registered.  You can perform JOINs, aggregations, sorting, etc.  For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, avg_rating: float, genres: array<string>, name: string, release_date: date, url: string, video_release_date: date]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql.sql(\"SELECT * from movies where name LIKE 'Rumble in the Bronx%'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your hand at a few queries.  Find the IDs of 3 movies you love.  For a more advanced challenge, get a list of all the movies made in the year you were born.  (Hint: LIKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets load our ratings up \n",
    "ratings = reader(\"ratings_by_user\")\n",
    "ratings.registerTempTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOINS and Aggregations\n",
    "\n",
    "Since we've put our movies and our ratings in tables, we can join them.  Pretty convenient.  We can do various JOINs.  By default, like a RDBMS, the inner join is used, but we also can do LEFT, RIGHT, FULL.  We also have unions and subqueries.  We can perform aggregations on our results as well.  We can take the results of any query (a DataFrame) and use it as a table for future queries.  This is incredibly powerful. \n",
    "\n",
    "Full docs: https://spark.apache.org/docs/latest/sql-programming-guide.html#compatibility-with-apache-hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                name|rating|\n",
      "+--------------------+------+\n",
      "|Secrets & Lies (1...|     5|\n",
      "|        Kolya (1996)|     5|\n",
      "|Sense and Sensibi...|     5|\n",
      "|      Titanic (1997)|     5|\n",
      "|         Emma (1996)|     5|\n",
      "|As Good As It Get...|     5|\n",
      "|L.A. Confidential...|     5|\n",
      "|Shall We Dance? (...|     5|\n",
      "|Good Will Hunting...|     5|\n",
      "|        Fargo (1996)|     5|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings = \" \".join([\"SELECT movies.name, ratings.rating from users\",\n",
    "                    \"JOIN ratings on users.id = ratings.user_id\",\n",
    "                    \"JOIN movies on ratings.movie_id = movies.id \",\n",
    "                    \"WHERE users.name = 'Dani Traphagen'\",\n",
    "                    \"ORDER BY rating DESC LIMIT 10\"])\n",
    "sql.sql(ratings).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
