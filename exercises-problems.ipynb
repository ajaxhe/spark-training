{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Connecting to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an iPython notebook.  You can execute a cell by clicking on it and pressing shift-enter.\n",
    "\n",
    "We can execute spark commands in here directly and get immediate results.\n",
    "\n",
    "We're going to be using Python with DataFrames, which is only available in Spark 1.3 or later.  We're going to be using a recent version of open source spark.  To use it, you'll have to import the `SQLContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Cassandra Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = sql.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=\"training\", table=\"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying results\n",
    "\n",
    "If we never perform an operation, our dataframe is never read in.  We can force our dataframe into memory and see it by calling `collect()` or `show()` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=1, age=34, favorite_foods=[u'Bacon', u'Cheese'], name=u'Jon'),\n",
       " Row(user_id=2, age=22, favorite_foods=[u'Kale', u'Pizza', u'Wine'], name=u'Dani'),\n",
       " Row(user_id=4, age=1, favorite_foods=[u'Candy', u'Fear'], name=u'Baby Luke'),\n",
       " Row(user_id=3, age=108, favorite_foods=[u'Muffins', u'Pie', u'Steak'], name=u'Patrick'),\n",
       " Row(user_id=5, age=10, favorite_foods=[u'Anger'], name=u'Larry')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------------+---------+\n",
      "|user_id|age|      favorite_foods|     name|\n",
      "+-------+---+--------------------+---------+\n",
      "|      1| 34|ArrayBuffer(Bacon...|      Jon|\n",
      "|      2| 22|ArrayBuffer(Kale,...|     Dani|\n",
      "|      4|  1|ArrayBuffer(Candy...|Baby Luke|\n",
      "|      3|108|ArrayBuffer(Muffi...|  Patrick|\n",
      "|      5| 10|  ArrayBuffer(Anger)|    Larry|\n",
      "+-------+---+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Filtering\n",
    "\n",
    "If we're going to do anything with our data, we need to be able to do a simple task: Filtering.\n",
    "\n",
    "Here's the syntax for filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, age: int, favorite_foods: array<string>, name: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.filter(user.age > 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an alternative syntax for filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, age: int, favorite_foods: array<string>, name: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user[user.age > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, a third syntax for filters that have a degree of complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=3, age=108, favorite_foods=[u'Muffins', u'Pie', u'Steak'], name=u'Patrick'),\n",
       " Row(user_id=5, age=10, favorite_foods=[u'Anger'], name=u'Larry')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.filter(\"age > 100 or name = 'Larry'\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try filtering for users name \"Jon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you refer to `user.age`, you're looking at a `Column`.  The api for `Column is here: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column\n",
    "\n",
    "**Advanced Query:** Try selecting the users who have the favorite food \"Bacon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A nicer reader\n",
    "\n",
    "Personally I find needing to code `org.apache.spark.sql.cassandra` everywhere a little annoying.  Here's a couple convenience functions that returns a function (slightly tricky) that can be used to reference tables in a keyspace.  Execute the below block.  You can then refer to tables like such:\n",
    "\n",
    "`user = reader(\"user\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(sql, keyspace):\n",
    "    def reader(table):\n",
    "        df = sql.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=keyspace, table=table)\n",
    "        return df\n",
    "    return reader\n",
    "\n",
    "def create_writer(sql, keyspace, mode=\"append\"):\n",
    "    def writer(df, table):\n",
    "        df.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "                 options(table=table, keyspace=keyspace).save(mode=\"append\")\n",
    "    return writer\n",
    "\n",
    "writer = create_writer(sql, \"training\")\n",
    "reader = create_reader(sql, \"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Migrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
